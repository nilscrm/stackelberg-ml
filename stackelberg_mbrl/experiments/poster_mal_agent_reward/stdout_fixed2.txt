Pretraining policy model
 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 501,760/500,000  [ 0:02:24 < 0:00:00 , 3,447 it/s ]
Pretraining Iteration 0
	Avg Reward (random model 0):      1.650 ± 0.436
	Avg Reward (random model 1):      0.440 ± 0.360
	Avg Reward (random model 2):      4.210 ± 4.612
	Avg Reward (random model 3):      0.800 ± 0.316
	Avg Reward (random model 4):      1.780 ± 1.154
	Avg Reward (random model 5):      2.320 ± 1.798
	Avg Reward (random model 6):      3.490 ± 6.637
	Avg Reward (random model 7):      1.210 ± 1.151
	Avg Reward (random model 8):      0.980 ± 0.024
	Avg Reward (random model 9):      1.440 ± 0.838
	Avg Reward (true env):      36.050 ± 1.252
	Avg Reward (eval env):   36.450 ± 1.073
Training world model
 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 501,760/500,000  [ 0:03:46 < 0:00:00 , 2,194 it/s ]
Model reward: (36.369999999180436, 1.0998181678136871)
Avg Policy Reward on learned model:   13.340 ± 9.742
Avg Policy Reward on learned model:   36.610 ± 0.784
